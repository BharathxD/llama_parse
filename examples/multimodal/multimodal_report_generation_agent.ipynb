{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ae9bad-b8cc-43de-ba7d-387e0155674c",
   "metadata": {},
   "source": [
    "# Multimodal Report Generation Agent \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_report_generation_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this cookbook we show you how to build a multimodal report generation agent from a bank of research reports. We use the a set of ICLR papers (which were also used as the dataset in our [DeepLearning.ai course](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/?utm_campaign=llamaindexC2-launch&utm_medium=headband&utm_source=dlai-homepage).\n",
    "\n",
    "We use our workflow abstraction to define an agentic system that first performs research to pull in the relevant files, and then surfaces it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8d9a7-5036-4d32-818f-00b2e888521f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccdd53-e68a-4199-aacb-cfe71ad1ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5556-a789-4386-a1ee-cce01dbeb6cf",
   "metadata": {},
   "source": [
    "### Setup Observability\n",
    "\n",
    "We setup an integration with LlamaTrace (integration with Arize).\n",
    "\n",
    "If you haven't already done so, make sure to create an account here: https://llamatrace.com/login. Then create an API key and put it in the `PHOENIX_API_KEY` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabee1f-290a-4c85-b362-54f45c8559ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U llama-index-callbacks-arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb245c-730b-4c34-ad68-708fdde0e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import llama_index.core\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ba6b0-51af-42f9-b1b2-8d3e721ef782",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "Setup models that will be used for downstream orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2071d-bbc2-4707-8ae7-cb4e1fecafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb362db-b1b1-4eea-be1a-b1f78b0779d7",
   "metadata": {},
   "source": [
    "## Load, Parse, and Index Research Papers\n",
    "\n",
    "Here we load 11 popular ICLR 2024 papers, and then we parse through LlamaParse.\n",
    "\n",
    "**NOTE**: this may be slow. To save your results, run the cell to save all your outputs to JSON, so you can reload instead of having to re-parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce3407-a7d2-47e8-9eaf-ab297a94750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\",\n",
    "]\n",
    "\n",
    "data_dir = \"iclr_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93b9c0-a343-4873-92de-60d8af2260f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{data_dir}\"\n",
    "for url, paper in zip(urls, papers):\n",
    "    !wget \"{url}\" -O \"{data_dir}/{paper}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fa3bd-c70f-45d5-9377-d81be8160891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee54b5-b423-4b43-930c-ea941723ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# delete and recreate the image directory if not already created\n",
    "out_image_dir = \"out_iclr_images\"\n",
    "!rm -rf \"{out_image_dir}\"\n",
    "!mkdir \"{out_image_dir}\"\n",
    "\n",
    "\n",
    "paper_dicts = {}\n",
    "\n",
    "for paper_path in papers:\n",
    "    paper_base = Path(paper_path).basename\n",
    "    full_paper_path = str(Path(data_dir) / paper_path)\n",
    "    print(paper_base)\n",
    "    raise Exception\n",
    "    md_json_objs = parser.get_json_result(full_paper_path)\n",
    "    json_dicts = md_json_objs[0][\"pages\"]\n",
    "\n",
    "    image_path = str(Path(out_image_dir) / paper_base)\n",
    "    image_dicts = parser.get_images(md_json_objs, download_path=image_path)\n",
    "    paper_dicts[paper_path] = {\n",
    "        \"paper_path\": full_paper_path,\n",
    "        \"json_dicts\": json_dicts,\n",
    "        \"image_path\": image_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae2dee-9d85-4604-8a51-705d4db527f7",
   "metadata": {},
   "source": [
    "#### Get Text Nodes\n",
    "\n",
    "Convert the dictionary above into TextNode objects that we can put into a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c24174-05ce-417f-8dd2-79c3f375db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e331dfe-a627-4e23-8c57-70ab1d9342e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these are utility functions to sort the dumped images by the page number\n",
    "# (they are formatted like \"{uuid}-{page_num}.jpg\"\n",
    "import re\n",
    "\n",
    "\n",
    "def get_page_number(file_name):\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    \"\"\"Get image files sorted by page.\"\"\"\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_page_number)\n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fe5ef-171e-4a54-9084-7a7805103a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# attach image metadata to the text nodes\n",
    "def get_text_nodes(json_dicts, paper_path, image_dir=None):\n",
    "    \"\"\"Split docs into nodes, by separator.\"\"\"\n",
    "    nodes = []\n",
    "\n",
    "    image_files = _get_sorted_image_files(image_dir) if image_dir is not None else None\n",
    "    md_texts = [d[\"md\"] for d in json_dicts]\n",
    "\n",
    "    for idx, md_text in enumerate(md_texts):\n",
    "        chunk_metadata = {\n",
    "            \"page_num\": idx + 1,\n",
    "            \"parsed_text_markdown\": md_text,\n",
    "            \"paper_path\": paper_path,\n",
    "        }\n",
    "        if image_files is not None:\n",
    "            image_file = image_files[idx]\n",
    "            chunk_metadata[\"image_path\"] = str(image_file)\n",
    "        chunk_metadata[\"parsed_text_markdown\"] = md_text\n",
    "        node = TextNode(\n",
    "            text=\"\",\n",
    "            metadata=chunk_metadata,\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591669c-5a8e-491d-9cef-0b754abbf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will combine all nodes from all papers into a single list\n",
    "all_text_nodes = []\n",
    "text_nodes_dict = {}\n",
    "for paper_path, paper_dict in paper_dicts.items():\n",
    "    json_dicts = paper_dict[\"json_dicts\"]\n",
    "    text_nodes = get_text_nodes(\n",
    "        json_dicts, paper_dict[\"paper_path\"], image_dir=paper_dict[\"image_path\"]\n",
    "    )\n",
    "    all_text_nodes.extend(text_nodes)\n",
    "    text_nodes_dict[paper_path] = text_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c13950-c1db-435f-b5b4-89d62b8b7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_num: 11\n",
      "image_path: data_images/412ac275-abe2-4585-be43-5680e7754740-page-10.jpg\n",
      "parsed_text_markdown: # Commitment to Disciplined Reinvestment Rate\n",
      "\n",
      "Disciplined Reinvestment Rate is the Foundation for Superior Returns on and of Capital, while Driving Durable CFO Growth\n",
      "\n",
      "| Metric | Value |\n",
      "|--------|-------|\n",
      "| 10-Year Reinvestment Rate | ~50% |\n",
      "| CFO CAGR 2024-2032 | ~6% |\n",
      "| Mid-Cycle Planning Price | at $60/BBL WTI |\n",
      "\n",
      "| Period | Industry Growth Focus | ConocoPhillips Strategy Reset | Reinvestment Rate |\n",
      "|--------|------------------------|-------------------------------|-------------------|\n",
      "| 2012-2016 | >100% Reinvestment Rate | - | ~$75/BBL WTI Average |\n",
      "| 2017-2022 | - | <60% Reinvestment Rate | ~$63/BBL WTI Average |\n",
      "| 2023E | - | - | at $80/BBL WTI |\n",
      "| 2024-2028 | - | - | at $60/BBL WTI (with $80/BBL WTI option shown) |\n",
      "| 2029-2032 | - | - | at $60/BBL WTI (with $80/BBL WTI option shown) |\n",
      "\n",
      "*Chart shows ConocoPhillips Average Annual Reinvestment Rate (%) over time, with historic rates in grey and projected rates in blue.*\n",
      "\n",
      "Reinvestment rate and cash from operations (CFO) are non-GAAP measures. Definitions and reconciliations are included in the Appendix.\n"
     ]
    }
   ],
   "source": [
    "print(all_text_nodes[10].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f404f56-db1e-4ed7-9ba1-ead763546348",
   "metadata": {},
   "source": [
    "### Build Indexes\n",
    "\n",
    "Once the text nodes are ready, we feed into our vector store index abstraction, which will index these nodes into a simple in-memory vector store (of course, you should definitely check out our 40+ vector store integrations!)\n",
    "\n",
    "Besides vector indexing, we **also** store a mapping of paper path to the summary index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7bc2e-be3d-4fd3-a1df-b2dcaa66c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    SummaryIndex,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# Vector Indexing\n",
    "if not os.path.exists(\"storage_nodes_papers\"):\n",
    "    index = VectorStoreIndex(text_nodes)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage_nodes_papers\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage_nodes_papers\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "    \n",
    "# Summary Index dictionary - store map from paper path to a summary index around it\n",
    "paper_summary_indexes = {\n",
    "    paper_path: SummaryIndex(text_nodes_dict[paper_path])\n",
    "    for paper_path in papers\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bebde3-015b-48b9-81f1-264ee6426a41",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We define two tools for the downstream agent: a chunk-level retriever tool and a document-retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1114bdc-3a2a-4703-b367-80e248469a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# function tools\n",
    "def chunk_retriever_fn(query: str) -> List[NodeWithScore]:\n",
    "    \"\"\"Retrieves a small set of relevant document chunks from the corpus.\n",
    "    \n",
    "    ONLY use for research questions that want to look up specific facts from the knowledge corpus,\n",
    "    and don't need entire documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=5)\n",
    "    nodes = retriever.retrieve(query)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "\n",
    "def _get_document_nodes(nodes: List[NodeWithScore], top_n: int = 2) -> List[NodeWithScore]:\n",
    "    \"\"\"Get document nodes from a set of chunk nodes.\n",
    "    \n",
    "    Given chunk nodes, \"de-reference\" into a set of documents, with a simple weighting function (cumulative total) to determine ordering.\n",
    "    \n",
    "    Cutoff by top_n.\n",
    "    \n",
    "    \"\"\"\n",
    "    paper_paths = {n.metadata[\"paper_path\"] for n in nodes}\n",
    "    paper_path_scores = {f: 0 for f in file_paths}\n",
    "    for n in nodes:\n",
    "        paper_path_scores[n.metadata[\"paper_path\"]] += n.score\n",
    "        \n",
    "    # Sort paper_path_scores by score in descending order\n",
    "    sorted_paper_paths = sorted(paper_path_scores.items(), key=itemgetter(1), reverse=True)\n",
    "    # Take top_n paper paths\n",
    "    top_paper_paths = [path for path, score in sorted_paper_paths[:top_n]]\n",
    "    \n",
    "    # use summary index to get nodes from all paper paths\n",
    "    all_nodes = []\n",
    "    for paper_path in top_paper_paths:\n",
    "        # NOTE: input to retriever can be blank \n",
    "        all_nodes.extend(paper_summary_indexes[paper_path].as_retriever().retrieve(\"\"))\n",
    "        \n",
    "    return all_nodes\n",
    "\n",
    "def doc_retriever_fn(query: str) -> float:\n",
    "    \"\"\"Document retriever that retrieves entire documents from the corpus.\n",
    "    \n",
    "    ONLY use for research questions that may require searching over entire research reports.\n",
    "    \n",
    "    Will be slower and more expensive than chunk-level retrieval but may be necessary.\n",
    "    \"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=5)\n",
    "    nodes = retriever.retrieve(query)\n",
    "    return _get_document_nodes(nodes)\n",
    "\n",
    "\n",
    "chunk_retriever_tool = FunctionTool.from_defaults(fn=chunk_retriever_fn)\n",
    "doc_retriever_tool = FunctionTool.from_defaults(fn=doc_retriever_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f4dcb-e569-47d7-8d35-19bc6f5ba12f",
   "metadata": {},
   "source": [
    "## Build Workflow \n",
    "\n",
    "Now that we've built the index, we're ready to build the report generation workflow. \n",
    "\n",
    "The workflow contains roughly the following steps: \n",
    "\n",
    "1. **Research Gathering**: Perform a function calling loop where the agent tries to reason about what tool to call (chunk-level or document-level retrieval) in order to gather more information. All information is shared to a dictionary that is propagated throughout each step. The tools return an indication of the type of information returned to the agent. After the agent feels like it's gathered enough information, move on to the next phase.\n",
    "\n",
    "\n",
    "2. **Report Generation**: Generate a research report given the pooled research. For now, try to stuff as much information into the context window through the summary index.\n",
    "\n",
    "\n",
    "This implementation is inspired by our [Function Calling Agent workflow](https://docs.llamaindex.ai/en/latest/examples/workflow/function_calling_agent/) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa654a0-4c43-48e3-85d1-b1796445fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Workflow\n",
    "\n",
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ChunkRetrievalEvent(Event):\n",
    "    query: str\n",
    "    \n",
    "    \n",
    "class DocRetrievalEvent(Event):\n",
    "    query: str\n",
    "    \n",
    "\n",
    "class ReportGenerationEvent(Event):\n",
    "    input: str\n",
    "\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "        self.summarizer = CompactAndRefine(llm=self.llm)\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step()\n",
    "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> ChunkRetrievalEvent | DocRetrievalEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "        if not tool_calls:\n",
    "            # all the content should be stored in the context, so just pass along input\n",
    "            return ReportGenerationEvent(input=ev.input)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.tool_name == \"chunk_retrieval\":\n",
    "                return ChunkRetrievalEvent(query=tool_call.tool_kwargs[\"query_str\"])\n",
    "            elif tool_call.tool_name == \"doc_retrieval\":\n",
    "                return DocRetrievalEvent(query=tool_call.tool_kwargs[\"query_str\"])\n",
    "            else:\n",
    "                return StopEvent(result={\"response\": \"Invalid tool.\"})\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def handle_retrieval(\n",
    "        self, ctx: Context, ev: ChunkRetrievalEvent | DocRetrievalEvent\n",
    "    ) -> InputEvent:\n",
    "        \"\"\"Handle retrieval.\n",
    "\n",
    "        Store retrieved chunks, and go back to agent reasoning loop.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(ev, ChunkRetrievalEvent):\n",
    "            retrieved_chunks = self.chunk_retriever_tool(ev.query).raw_output\n",
    "        else:\n",
    "            retrieved_chunks = self.doc_retriever_tool(ev.query).raw_output\n",
    "        ctx.data[\"stored_chunks\"].extend(retrieved_chunks)\n",
    "\n",
    "        # synthesize an answer given the query to return to the LLM.\n",
    "        response = self.summarizer.synthesize(ev.query, nodes=retrieved_chunks)\n",
    "        self.memory.put(str(response))\n",
    "\n",
    "        # send input event back with updated chat history\n",
    "        return InputEvent(input=self.memory.get())\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(\n",
    "        self, ctx: Context, ev: ReportGenerationEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Generate report.\"\"\"\n",
    "        # given all the context, generate query\n",
    "        self.summarizer.synthesize(ev.query, nodes=ctx[\"stored_chunks\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
