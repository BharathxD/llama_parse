{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c79c38-38a3-40f3-ba2e-250649347d63",
   "metadata": {},
   "source": [
    "# Multimodal Parsing using Anthropic Claude (Sonnet 3.5)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This cookbook shows you how to use LlamaParse to parse any document with the multimodal capabilities of Sonnet 3.5. \n",
    "\n",
    "LlamaParse allows you to plug in external, multimodal model vendors for parsing - we handle the error correction, validation, and scalability/reliability for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e60ecf-519c-41fc-911b-765adaf8bad4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a9e532-1454-40e0-bbf0-fd442c350121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fb0aa-74cd-476f-8161-efd9e04248bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-11 23:44:38--  https://arxiv.org/pdf/2307.09288\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  69.3MB/s    in 0.2s    \n",
      "\n",
      "2024-07-11 23:44:38 (69.3 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://arxiv.org/pdf/2307.09288\" -O data/llama2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29a9d7-5bd9-4fb8-8ec1-4c128a748662",
   "metadata": {},
   "source": [
    "## Initialize LlamaParse\n",
    "\n",
    "Initialize LlamaParse in multimodal mode, and specify the vendor.\n",
    "\n",
    "**NOTE**: optionally you can specify the Anthropic API key. If you do so you will be charged our base LlamaParse price of 0.3c per page. If you don't then you will be charged 6c per page, as we will make the calls to Claude for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc921729-3446-42ca-8e1b-a6fd26195ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_text_nodes(json_list: List[dict]):\n",
    "    text_nodes = []\n",
    "    for idx, page in enumerate(json_list):\n",
    "        text_node = TextNode(\n",
    "            text=page[\"text\"],\n",
    "            metadata={\n",
    "                \"page\": page[\"page\"]\n",
    "            }\n",
    "        )\n",
    "        text_nodes.append(text_node)\n",
    "    return text_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e9d9cf-8189-4fcb-b34f-cde6cc0b59c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal=True, \n",
    "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\",\n",
    "    # parsing_instruction=\"Output markdown tables such that all the vertical/horizontal bars align nicely in a 2D grid.\",\n",
    "    invalidate_cache=True\n",
    ")\n",
    "# json_objs = parser.get_json_result(\"./data/llama2.pdf\")\n",
    "# json_list = json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08e80bc-c6ba-451f-869c-5f51bf7ab941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 2411cd53-a39b-416f-9003-676ba968c3f4\n"
     ]
    }
   ],
   "source": [
    "docs = parser.load_data(\"./data/llama2-p33.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e67c212-740d-4307-87a8-55e1ac66e35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 21: RLHF learns to adapt the temperature with regard to the type of prompt. Lower Self-BLEU corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more diversity when generating responses to creative prompts. We prompt each model with a diverse set of 10 creative and 10 factual instructions and sample 25 responses. This is repeated for the temperatures T ∈ {k/10 | k ∈ ℕ : 1 ≤ k ≤ 15}. For each of the 25 responses we compute the Self-BLEU metric and report the mean and standard deviation against the temperature.\n",
      "\n",
      "Date: 01/01/2023 Year: 2023 Year: 852 Cutting knowledge: 01/01/1940\n",
      "\n",
      "|Is the Earth flat Or round?|The Earth is roundThe idea that the Earth is flat is a myth that has been disproven by scientific evidence and observationsThe Earth's shape has been confirmed through a variety of methods; including satellite imagery; GPS; oblate spheroid, meaning it is slightly flattened at the poles and bulging at the equator; space exploration; However, there were also some ancient Greeks, such as Eratosthenes; who believed that the Earth was round and were able to make calculations suggesting that it was a sphere. It's worth noting that the idea of a flat Earth was not universally accepted in 852, and there were many people who believed in a round Earth. Additionally, as time went on, the idea of a round Earth became more widely accepted; and by the Renaissance, the concept of a flat Earth had largely fallen out of favor.|\n",
      "|---|---|\n",
      "|Who won the 2nd World War?|In 852, the Earth was believed to be flat by many people. The idea of a flat Earth was supported by various religious and philosophical beliefs as well as by some scientific observations that suggested the Earth was flat. However, there were many people who believed in a round Earth. Additionally, as time went on, the idea of a round Earth became more widely accepted; and by the Renaissance, the concept of a flat Earth had largely fallen out of favor.|\n",
      "\n",
      "Figure 22: Time awareness — illustration of our model generalizing the notion of time, with 1,000 SFT time-focused data.\n",
      "\n",
      "Temporal Perception Our model showcased impressive generalization ability, as shown in Figure 22. We manually tested dozens of examples and observed consistently that our model demonstrates a robust capability to organize its knowledge in a temporal manner, even when provided with minimal data. To instill a concept of time in L-C, we collected a set of 1,000 SFT examples that were related to specific dates. These examples included questions like “How long ago did Barack Obama become president?” Each was associated with two critical pieces of metadata: the date when the query was posed — which influenced the response — and the event date, a point in time prior to which the question would be nonsensical. The observation suggests that LLMs have internalized the concept of time to a greater extent than previously assumed, despite their training being solely based on next-token prediction and data that is randomly shuffled without regard to their chronological context.\n",
      "\n",
      "Tool Use Emergence The integration of LLMs with tools is a growing research area, as highlighted in Mialon et al. (2023). The approach devised in Toolformer (Schick et al., 2023) entails the sampling of millions\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2dce0-a2a2-4e6f-afe0-fe9501dd1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_list[32][\"md\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c51b0-7878-48d7-9bc3-02b516500128",
   "metadata": {},
   "source": [
    "### Setup GPT-4o baseline\n",
    "\n",
    "For comparison, we will also parse the document using GPT-4o (3c per page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3f258-50ae-4988-b904-c105463a498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser_gpt4o = LlamaParse(\n",
    "    use_multimodal_model=True, \n",
    "    vendor_multimodal_model=\"openai-gpt4o\",\n",
    "    parsing_instruction=\"Output markdown tables such that all the vertical/horizontal bars align nicely in a 2D grid.\",\n",
    "    invalidate_cache=True\n",
    ")\n",
    "json_objs_gpt4o = parser_gpt4o.get_json_result(\"./data/llama2.pdf\")\n",
    "json_list_gpt4o = json_objs_gpt4o[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47f04e-12e1-4c80-a71d-ef7721f96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_list_gpt4o[32][\"md\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c20f7a-2901-4dd0-b635-a4b33c5664c1",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "Let's visualize the results along with the original document page.\n",
    "\n",
    "We see that Sonnet is able to extract complex visual elements like graphs in way more detail! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778698aa-da7e-4081-b3b5-0372f228536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Sonnet-3.5\n",
    "print(docs[32].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511a30f-3efc-4142-9668-7dc056a24d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPT-4o\n",
    "print(docs_gpt4o[32].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f7729-fa0f-4ca0-8562-c42afeaa8532",
   "metadata": {},
   "source": [
    "## Setup RAG Pipeline\n",
    "\n",
    "These parsing capabilities translate to great RAG performance as well. Let's setup a RAG pipeline over this data.\n",
    "\n",
    "(we'll use GPT-4o from OpenAI for the actual text synthesis step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60972d7a-7948-4ad7-89df-57004acee917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index = SummaryIndex.from_documents(docs)\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "index_gpt4o = SummaryIndex.from_documents(docs_gpt4o)\n",
    "query_engine_gpt4o = index_gpt4o.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df7bcb-1df4-4a01-88fc-2d596b1cc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me more about all the values for each line in the RLHF graph.\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "response_gpt4o = query_engine_gpt4o.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_parse",
   "language": "python",
   "name": "llama_parse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
