{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c79c38-38a3-40f3-ba2e-250649347d63",
   "metadata": {},
   "source": [
    "# Multimodal Parsing using Anthropic Claude (Sonnet 3.5)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This cookbook shows you how to use LlamaParse to parse any document with the multimodal capabilities of Sonnet 3.5. \n",
    "\n",
    "LlamaParse allows you to plug in external, multimodal model vendors for parsing - we handle the error correction, validation, and scalability/reliability for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e60ecf-519c-41fc-911b-765adaf8bad4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9e532-1454-40e0-bbf0-fd442c350121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fb0aa-74cd-476f-8161-efd9e04248bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-11 23:44:38--  https://arxiv.org/pdf/2307.09288\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.3.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  69.3MB/s    in 0.2s    \n",
      "\n",
      "2024-07-11 23:44:38 (69.3 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://arxiv.org/pdf/2307.09288\" -O data/llama2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29a9d7-5bd9-4fb8-8ec1-4c128a748662",
   "metadata": {},
   "source": [
    "## Initialize LlamaParse\n",
    "\n",
    "Initialize LlamaParse in multimodal mode, and specify the vendor.\n",
    "\n",
    "**NOTE**: optionally you can specify the Anthropic API key. If you do so you will be charged our base LlamaParse price of 0.3c per page. If you don't then you will be charged 6c per page, as we will make the calls to Claude for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc921729-3446-42ca-8e1b-a6fd26195ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_text_nodes(json_list: List[dict]):\n",
    "    text_nodes = []\n",
    "    for idx, page in enumerate(json_list):\n",
    "        text_node = TextNode(text=page[\"text\"], metadata={\"page\": page[\"page\"]})\n",
    "        text_nodes.append(text_node)\n",
    "    return text_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9d9cf-8189-4fcb-b34f-cde6cc0b59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\",\n",
    "    # parsing_instruction=\"Output any charts in a well-formatted markdown table such that all the vertical/horizontal bars align nicely in a 2D grid.\",\n",
    "    invalidate_cache=True,\n",
    ")\n",
    "# json_objs = parser.get_json_result(\"./data/llama2-p33.pdf\")\n",
    "# json_list = json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e80bc-c6ba-451f-869c-5f51bf7ab941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 6baaaba1-3f58-4fde-92d3-39c0a6ffb254\n"
     ]
    }
   ],
   "source": [
    "docs = parser.load_data(\"./data/llama2-p33.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67c212-740d-4307-87a8-55e1ac66e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image contains two figures and accompanying text. I'll describe each component in markdown format:\n",
      "\n",
      "![Graphs showing RLHF adaptation to temperature for factual and creative prompts](image_url_here)\n",
      "\n",
      "Figure 21: RLHF learns to adapt the temperature with regard to the type of prompt. Lower Self-BLEU corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more diversity when generating responses to creative prompts. We prompt each model with a diverse set of 10 creative and 10 factual instructions and sample 25 responses. This is repeated for the temperatures T ∈ {k/10 | k ∈ N : 1 ≤ k ≤ 15}. For each of the 25 responses we compute the Self-BLEU metric and report the mean and standard deviation against the temperature.\n",
      "\n",
      "![Example of time awareness in AI responses](image_url_here)\n",
      "\n",
      "Figure 22: Time awareness — illustration of our model generalizing the notion of time, with 1,000 SFT time-focused data.\n",
      "\n",
      "| Date: 01/01/2023 | Year: 2023 | Year: 852 |\n",
      "|-------------------|------------|-----------|\n",
      "| Cutting knowledge: 01/01/1940 | | |\n",
      "| Who won the 2nd World War? | Is the Earth flat or round? | Is the Earth flat or round? |\n",
      "| I'm not sure what you're referring to, as my knowledge stopped in 1940. | The Earth is round. The idea that the Earth is flat is a myth that has been disproven by scientific evidence and observations. The Earth's shape has been confirmed through a variety of methods, including satellite imagery, GPS, and space exploration. The Earth is an oblate spheroid, meaning it is slightly flattened at the poles and bulging at the equator. | In 852, the Earth was believed to be flat by many people. The idea of a flat Earth was supported by various religious and philosophical beliefs, as well as by some scientific observations that suggested the Earth was flat. However, there were also some ancient Greeks and other scholars who believed that the Earth was round and were able to make calculations suggesting that it was a sphere. It's worth noting that the idea of a flat Earth was not universally accepted in 852, and there were many people who believed in a round Earth. However, as time went on, the idea of a round Earth became more widely accepted, and by the Renaissance, the concept of a flat Earth had largely fallen out of favor. |\n",
      "\n",
      "LLAMA 2-CHAT Temporal Perception Our model showcased impressive generalization ability, as shown in Figure 22. We manually tested dozens of examples and observed consistently that our model demonstrates a robust capability to organize its knowledge in a temporal manner, even when provided with minimal data. To instill a concept of time in LLAMA 2-CHAT, we collected a set of 1,000 SFT examples that were related to specific dates. These examples included questions like \"How long ago did Barack Obama become president?\" Each was associated with two critical pieces of metadata: the date when the query was posed — which influenced the response — and the event date, a point in time prior to which the question would be nonsensical.\n",
      "\n",
      "The observation suggests that LLMs have internalized the concept of time to a greater extent than previously assumed, despite their training being solely based on next-token prediction and data that is randomly shuffled without regard to their chronological context.\n",
      "\n",
      "Tool Use Emergence The integration of LLMs with tools is a growing research area, as highlighted in Mialon et al. (2023). The approach devised in Toolformer (Schick et al., 2023) entails the sampling of millions\n",
      "\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2dce0-a2a2-4e6f-afe0-fe9501dd1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c51b0-7878-48d7-9bc3-02b516500128",
   "metadata": {},
   "source": [
    "### Setup GPT-4o baseline\n",
    "\n",
    "For comparison, we will also parse the document using GPT-4o (3c per page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3f258-50ae-4988-b904-c105463a498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id ab86c158-5b10-4086-81de-4741b7cc4b2e\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser_gpt4o = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model=\"openai-gpt4o\",\n",
    "    # parsing_instruction=\"Output markdown tables such that all the vertical/horizontal bars align nicely in a 2D grid.\",\n",
    "    invalidate_cache=True,\n",
    ")\n",
    "json_objs_gpt4o = parser_gpt4o.get_json_result(\"./data/llama2-p33.pdf\")\n",
    "json_list_gpt4o = json_objs_gpt4o[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47f04e-12e1-4c80-a71d-ef7721f96401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Figure 21: RLHF learns to adapt the temperature with regard to the type of prompt.\n",
      "\n",
      "Lower Self-BLEU corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more diversity when generating responses to creative prompts. We prompt each model with a diverse set of 10 creative and 10 factual instructions and sample 25 responses. This is repeated for the temperatures \\( T \\in \\{k/10 \\mid k \\in \\mathbb{N} : 1 \\leq k \\leq 15\\} \\). For each of the 25 responses we compute the Self-BLEU metric and report the mean and standard deviation against the temperature.\n",
      "\n",
      "| Temperature | Factual Prompts (Self-BLEU) | Creative Prompts (Self-BLEU) |\n",
      "|-------------|------------------------------|------------------------------|\n",
      "| 0.4         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "| 0.6         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "| 0.8         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "| 1.0         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "| 1.2         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "| 1.4         | RLHF v3, RLHF v2, RLHF v1, SFT | RLHF v3, RLHF v2, RLHF v1, SFT |\n",
      "\n",
      "# Figure 22: Time awareness\n",
      "\n",
      "Illustration of our model generalizing the notion of time, with 1,000 SFT time-focused data.\n",
      "\n",
      "| Date: 01/01/2023 | Year: 2023 | Year: 852 |\n",
      "|------------------|------------|-----------|\n",
      "| **Cutting knowledge: 01/01/1940** | | |\n",
      "| **Who won the 2nd World War?** | **Is the Earth flat or round?** | **Is the Earth flat or round?** |\n",
      "| I'm not sure what you're referring to, as my knowledge stopped in 1940. | The Earth is round. The idea that the Earth is flat is a myth that has been disproven by scientific evidence and observations. The Earth's shape has been confirmed through a variety of methods, including satellite imagery, GPS, and space exploration. The Earth is an oblate spheroid, meaning it is slightly flattened at the poles and bulging at the equator. | In 852, the Earth was believed to be flat by many people. The idea of a flat Earth was supported by various religious and philosophical beliefs, as well as some scientific theories that suggested the Earth was flat. However, there were also some scholars who proposed that the Earth was round and were able to make calculations suggesting that it was a sphere. It's worth noting that the idea of a flat Earth was not universally accepted in 852, and there were people who believed in a round Earth. Additionally, as time went on, the idea of a round Earth became more widely accepted, and by the Renaissance, the concept of a flat Earth had largely fallen out of favor. |\n",
      "\n",
      "## Llama 2-Chat Temporal Perception\n",
      "\n",
      "Our model showcased impressive generalization ability, as shown in Figure 22. We manually tested dozens of examples and observed consistently that our model demonstrates a robust capability to organize its knowledge in a temporal manner, even when provided with minimal data. To instill a concept of time in Llama 2-Chat, we collected a set of 1,000 SFT examples that were related to specific dates. These examples included questions like \"How long ago did Barack Obama become president?\" Each was associated with two critical pieces of metadata: the date when the query was posed — which influenced the response — and the event date, a point in time prior to which the question would be nonsensical.\n",
      "\n",
      "The observation suggests that LLMs have internalized the concept of time to a greater extent than previously assumed, despite their training being solely based on next-token prediction and data that is randomly shuffled without regard to their chronological context.\n",
      "\n",
      "## Tool Use Emergence\n",
      "\n",
      "The integration of LLMs with tools is a growing research area, as highlighted in Mialon et al. (2023). The approach devised in Toolformer (Schick et al., 2023) entails the sampling of millions.\n"
     ]
    }
   ],
   "source": [
    "print(json_list_gpt4o[0][\"md\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c20f7a-2901-4dd0-b635-a4b33c5664c1",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "Let's visualize the results along with the original document page.\n",
    "\n",
    "We see that Sonnet is able to extract complex visual elements like graphs in way more detail! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778698aa-da7e-4081-b3b5-0372f228536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Sonnet-3.5\n",
    "print(docs[32].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511a30f-3efc-4142-9668-7dc056a24d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPT-4o\n",
    "print(docs_gpt4o[32].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f7729-fa0f-4ca0-8562-c42afeaa8532",
   "metadata": {},
   "source": [
    "## Setup RAG Pipeline\n",
    "\n",
    "These parsing capabilities translate to great RAG performance as well. Let's setup a RAG pipeline over this data.\n",
    "\n",
    "(we'll use GPT-4o from OpenAI for the actual text synthesis step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60972d7a-7948-4ad7-89df-57004acee917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index = SummaryIndex.from_documents(docs)\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "index_gpt4o = SummaryIndex.from_documents(docs_gpt4o)\n",
    "query_engine_gpt4o = index_gpt4o.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df7bcb-1df4-4a01-88fc-2d596b1cc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me more about all the values for each line in the RLHF graph.\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "response_gpt4o = query_engine_gpt4o.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_parse",
   "language": "python",
   "name": "llama_parse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
