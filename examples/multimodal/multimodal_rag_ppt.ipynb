{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ae9bad-b8cc-43de-ba7d-387e0155674c",
   "metadata": {},
   "source": [
    "# Building a Natively Multimodal RAG Pipeline (over a Slide Deck)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_ppt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this cookbook we show you how to build a multimodal RAG pipeline over a slide deck, with text, tables, images, diagrams, and complex layouts.\n",
    "\n",
    "A gap of text-based RAG is that they struggle with purely text-based representations of complex documents. For instance, if a page contains a lot of images and diagrams, a text parser would need to rely on raw OCR to extract out text. You can also use a multimodal model (e.g. gpt-4o and up) to do text extraction, but this is inherently a lossy conversion.\n",
    "\n",
    "Instead a **native multimodal pipeline** stores both a text and image representation of a document chunk. They are indexed via embeddings (text or image), and during synthesis both text and image are directly fed to the multimodal model for synthesis.\n",
    "\n",
    "This can have the following advantages:\n",
    "- **Robustness**: This solution is more robust than a pure text or even a pure image-based approach. In a pure text RAG approach, the parsing piece can be lossy. In a pure image-based approach, multimodal OCR is not perfect and may lose out against text parsing for text-heavy documents.\n",
    "- **Cost Optimization**: You may choose to dynamically include text-only, or text + image depending on the content of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8d9a7-5036-4d32-818f-00b2e888521f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccdd53-e68a-4199-aacb-cfe71ad1ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5556-a789-4386-a1ee-cce01dbeb6cf",
   "metadata": {},
   "source": [
    "### Setup Observability\n",
    "\n",
    "We setup an integration with LlamaTrace (integration with Arize).\n",
    "\n",
    "If you haven't already done so, make sure to create an account here: https://llamatrace.com/login. Then create an API key and put it in the `PHOENIX_API_KEY` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabee1f-290a-4c85-b362-54f45c8559ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U llama-index-callbacks-arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb245c-730b-4c34-ad68-708fdde0e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Arize Phoenix for logging/observability\n",
    "import llama_index.core\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb362db-b1b1-4eea-be1a-b1f78b0779d7",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Here we load the [Conoco Phillips 2023 investor meeting slide deck](https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce3407-a7d2-47e8-9eaf-ab297a94750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-10 22:07:20--  https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf\n",
      "Resolving static.conocophillips.com (static.conocophillips.com)... 2600:9000:25ef:e200:13:a3a2:1e40:93a1, 2600:9000:25ef:2400:13:a3a2:1e40:93a1, 2600:9000:25ef:6400:13:a3a2:1e40:93a1, ...\n",
      "Connecting to static.conocophillips.com (static.conocophillips.com)|2600:9000:25ef:e200:13:a3a2:1e40:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41895745 (40M) [application/pdf]\n",
      "Saving to: ‘data/conocophillips.pdf’\n",
      "\n",
      "data/conocophillips 100%[===================>]  39.95M  32.8MB/s    in 1.2s    \n",
      "\n",
      "2024-07-10 22:07:21 (32.8 MB/s) - ‘data/conocophillips.pdf’ saved [41895745/41895745]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data_images\n",
    "!wget \"https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf\" -O data/conocophillips.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ba6b0-51af-42f9-b1b2-8d3e721ef782",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "Setup models that will be used for downstream orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2071d-bbc2-4707-8ae7-cb4e1fecafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6416f-f580-4722-aaa9-7f3500408547",
   "metadata": {},
   "source": [
    "## Use LlamaParse to Parse Text and Images\n",
    "\n",
    "In this example, use LlamaParse to parse both the text and images from the document.\n",
    "\n",
    "We parse out the text in two ways: \n",
    "- in regular `text` mode using our default text layout algorithm\n",
    "- in `markdown` mode using GPT-4o (`gpt4o_mode=True`). This also allows us to capture page screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570089e5-238a-4dcc-af65-96e7393c2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "parser_text = LlamaParse(result_type=\"text\")\n",
    "parser_gpt4o = LlamaParse(result_type=\"markdown\", gpt4o_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82a985-4088-4bb7-9a21-0318e1b9207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-71c5-46bd-add9-ca5f4d378899\n"
     ]
    }
   ],
   "source": [
    "# docs_text = parser_text.load_data(\"data/conocophillips.pdf\")\n",
    "json_objs = parser_gpt4o.get_json_result(\"data/conocophilips.pdf\")\n",
    "json_list = json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506b603-c01f-45de-b354-4a0728dde03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_text[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318fb7b-fe6a-4a8a-b82e-4ed7b4512c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_list[10][\"md\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46a73e-c6e2-4b0b-bd10-31b0d3e4b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_list[1][\"md\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadb16c-97eb-4622-9551-b34d7f90d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dicts = parser_gpt4o.get_images(json_objs, download_path=\"data_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e098b-0606-4429-b48d-d4fe0140fc0e",
   "metadata": {},
   "source": [
    "## Build Multimodal Index\n",
    "\n",
    "In this section we build the multimodal index over the parsed deck. \n",
    "\n",
    "We do this by creating **text** nodes from the document that contain metadata referencing the original image path.\n",
    "\n",
    "In this example we're indexing the text node for retrieval. The text node has a reference to both the parsed text as well as the image screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae2dee-9d85-4604-8a51-705d4db527f7",
   "metadata": {},
   "source": [
    "#### Get Text Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c24174-05ce-417f-8dd2-79c3f375db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e331dfe-a627-4e23-8c57-70ab1d9342e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pages loaded through llamaparse\n",
    "import re\n",
    "\n",
    "\n",
    "def get_page_number(file_name):\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    \"\"\"Get image files sorted by page.\"\"\"\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_page_number)\n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fe5ef-171e-4a54-9084-7a7805103a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# attach image metadata to the text nodes\n",
    "def get_text_nodes(docs, image_dir=None, json_dicts=None):\n",
    "    \"\"\"Split docs into nodes, by separator.\"\"\"\n",
    "    nodes = []\n",
    "\n",
    "    image_files = _get_sorted_image_files(image_dir) if image_dir is not None else None\n",
    "    md_texts = [d[\"md\"] for d in json_dicts] if json_dicts is not None else None\n",
    "\n",
    "    doc_chunks = docs[0].text.split(\"---\")\n",
    "    for idx, doc_chunk in enumerate(doc_chunks):\n",
    "        chunk_metadata = {\"page_num\": idx + 1}\n",
    "        if image_files is not None:\n",
    "            image_file = image_files[idx]\n",
    "            chunk_metadata[\"image_path\"] = str(image_file)\n",
    "        if md_texts is not None:\n",
    "            chunk_metadata[\"parsed_text_markdown\"] = md_texts[idx]\n",
    "        #             chunk_metadata[\"parsed_text_markdown\"] = f\"\"\"\n",
    "        # Here is the parsed markdown. Parsing may be incorrect:\n",
    "        # {md_texts[idx]}\n",
    "        # -----\n",
    "        # \"\"\"\n",
    "        chunk_metadata[\"parsed_text\"] = doc_chunk\n",
    "        node = TextNode(\n",
    "            # text=doc_chunk,\n",
    "            # text=chunk_metadata[\"text_markdown\"],\n",
    "            text=\"\",\n",
    "            metadata=chunk_metadata,\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591669c-5a8e-491d-9cef-0b754abbf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will split into pages\n",
    "text_nodes = get_text_nodes(docs_text, image_dir=\"data_images\", json_dicts=json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c13950-c1db-435f-b5b4-89d62b8b7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_nodes[10].get_content(metadata_mode=\"llm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f404f56-db1e-4ed7-9ba1-ead763546348",
   "metadata": {},
   "source": [
    "#### Build Index\n",
    "\n",
    "Once the text nodes are ready, we feed into our vector store index abstraction, which will index these nodes into a simple in-memory vector store (of course, you should definitely check out our 40+ vector store integrations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea53c31-0e38-421c-8d9b-0e3adaa1677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(text_nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e33a4-9422-498d-87ee-d917bdf74d80",
   "metadata": {},
   "source": [
    "## Build Multimodal Query Engine\n",
    "\n",
    "We now use LlamaIndex abstractions to build a **custom query engine**. In contrast to a standard RAG query engine that will retrieve the text node and only put that into the prompt (response synthesis module), this custom query engine will also load the image document, and put both the text and image document into the response synthesis module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a94be2-e289-41a6-92e4-d3cb428fb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine, SimpleMultiModalQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import ImageNode, QueryBundle, NodeWithScore\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "gpt_4o = OpenAIMultiModal(model=\"gpt-4o\", max_new_tokens=4096)\n",
    "\n",
    "\n",
    "QA_PROMPT_TMPL = \"\"\"\\\n",
    "Below we give parsed text from slides in two different formats, as well as the image.\n",
    "\n",
    "We parse the text in both 'markdown' mode as well as 'raw text' mode. Markdown mode attempts \\\n",
    "to convert relevant diagrams into tables, whereas raw text tries to maintain the rough spatial \\\n",
    "layout of the text.\n",
    "\n",
    "Use the markdown context, but keep in mind the parsed data may be incorrect. Try to correlate\n",
    "data from text mode and the image to markdown. If you notice inconsistencies, then rely on your understanding\n",
    "of text mode and the image to give the response instead.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. Explain whether you got the answer\n",
    "from the parsed markdown or raw text or image, and if there's discrepancies, and your reasoning for the final answer.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(QA_PROMPT_TMPL)\n",
    "\n",
    "\n",
    "class MultimodalQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Custom multimodal Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    multi_modal_llm: OpenAIMultiModal\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "\n",
    "        # create ImageNode items from text nodes\n",
    "        image_nodes = [\n",
    "            NodeWithScore(node=ImageNode(image_path=n.metadata[\"image_path\"]))\n",
    "            for n in nodes\n",
    "        ]\n",
    "\n",
    "        # TODO: we just use the `synthesize` mode from `SimpleMultiModalQueryEngine`\n",
    "        # which synthesizes information from a query string and a set of text and image chunks.\n",
    "        query_engine = SimpleMultiModalQueryEngine(\n",
    "            self.retriever,  ## unused\n",
    "            multi_modal_llm=self.multi_modal_llm,\n",
    "            text_qa_template=qa_prompt,\n",
    "        )\n",
    "        response = query_engine.synthesize(\n",
    "            QueryBundle(query_str=query_str),\n",
    "            nodes + image_nodes\n",
    "            # image_nodes ## TMP\n",
    "        )\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890be59-fb12-4bb5-959b-b2d9600f7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = MultimodalQueryEngine(\n",
    "    retriever=index.as_retriever(similarity_top_k=9), multi_modal_llm=gpt_4o\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92aa4f1-7501-4711-b054-f02338e54e74",
   "metadata": {},
   "source": [
    "### Define Baseline\n",
    "\n",
    "In addition, we define a \"baseline\" where we rely only on text-based indexing. Here we can directly do `index.as_query_engine` to get back the \"default\" query engine over the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcfbc6-4e9b-41ad-ad81-1c4245b95cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_query_engine = index.as_query_engine(llm=llm, similarity_top_k=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9233c9-f110-4931-9d4e-745cce476b2b",
   "metadata": {},
   "source": [
    "## Build a Multimodal Agent\n",
    "\n",
    "Build an agent around the multimodal query engine. This gives you agent capabilities like query planning/decomposition and memory around a central QA interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc06de-5084-451a-ba4b-160e3587ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"vector_tool\",\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the data. Do NOT select if question asks for a summary of the data.\"\n",
    "    ),\n",
    ")\n",
    "agent = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool], llm=llm, verbose=True\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5f8ea-c463-413b-b8d3-d073df5baae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a similar agent for the baseline\n",
    "base_vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=base_query_engine,\n",
    "    name=\"vector_tool\",\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the data. Do NOT select if question asks for a summary of the data.\"\n",
    "    ),\n",
    ")\n",
    "base_agent = FunctionCallingAgentWorker.from_tools(\n",
    "    [base_vector_tool], llm=llm, verbose=True\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af72a4-24d0-4ce4-990c-5444edf98dea",
   "metadata": {},
   "source": [
    "## Try out Queries\n",
    "\n",
    "Let's try out queries against these documents and compare against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f23349-6dd9-4d0d-952c-343ba871a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\"<INSERT QUESTION HERE>\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
